# -*- coding: utf-8 -*-
"""FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zZ2baHMupS-hGnG-nIWi5saqSFiEfysM
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import impute
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

df = pd.read_csv('healthy_lifestyle_city_2021.csv')
print(df.describe())
df['Obesity levels(Country)'] = df['Obesity levels(Country)'].str.rstrip('%').astype('float')
df['Cost of a bottle of water(City)'] = df['Cost of a bottle of water(City)'].str.replace('£', '')
df['Cost of a monthly gym membership(City)'] = df['Cost of a monthly gym membership(City)'].str.replace('£', '')
df.head()

#correlation
corrmat = df.corr()
f, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(corrmat, annot=True, vmax=.8, square=True);

#scatter plots
x=df['City']
for column in df[1:]:
  y=df[column]
  plt.rcParams["figure.figsize"] = [15, 7.5]
  plt.rcParams["figure.autolayout"] = True
  plt.scatter(x, y, cmap="copper")
  plt.xlabel('City')
  plt.ylabel(column)
  plt.show()

#missing data
#print(df.isna().sum())
df1 = df[df != "-"]
df1.dropna(axis=0)
df2 = df1.dropna(axis=0)
print(df.isna().sum())
print(df2)

#outliers and categorical variables
ncols = df2.select_dtypes(include=[np.number])
ccols = df2.select_dtypes(exclude=[np.number])
fcol = df2
bestcities = df2  
#fcol.drop(ncols)
for column in ncols:
  ncols[column].describe()
  OL=ncols[column].quantile(0.25)
  OG=ncols[column].quantile(0.75)
  OR=OG-OL #OUTLIER RANGE= OUTLIERGREATER-OUTLIERLOWER

  llim=OL-(1.5*OR)
  glim=OG+(1.5*OR)
  print(llim, glim)
  dfl=(ncols[column]<llim)
  dfo=(ncols[column]>glim)
  fcol[column]=ncols[column][~(dfl | dfo)]
  #bestcities[column]=ncols[column][dfo]
  #print(fcol)
#fcol = fcol.dropna(axis=0)
#print(fcol)
fcol = fcol.dropna()
print(fcol)
dff = pd.concat([fcol,ccols])

#the final result
final = pd.concat([fcol, ccols]).drop_duplicates()
final = final.dropna()
#print(ccols.head())
#print(fcol.head())
print(final)
bestcities=bestcities.dropna()
#print(dff)
#print(bestcities)

"""I chose a data set that uses a variety of different data to assess what the best city to be in would be. I dont know if I will continue to use this dataset because cleaning the data and removing the outliers only finds the most average of cities. Thus why I included a bestcities dataframe so that one could chose where their perfect place to live would be. I removed the '%' and the 'Euro' sign from the columns that had them so that they were marked as numerical data. The best cities are those who have the most options and the highest happiness levels but are also the ones that are most expensive. The worst cities are on the opposite side of the spectrum. If I were to continue using this data set I think it would be fair to seperately analyze each column for the desired quartile range for the perfect experience. Example would be one with a high happiness level, a low monthly gym membership cost, and one with a medium level of average rain fall.

if the differences between the observations and the predicted values are small and unbiased.
"""

dft = final.iloc[:, 1:]
print(dft.shape)
#print(dft)
#xtest, train = train_test_split(dft, test_size=0.5)
#print(xtest.shape, train.shape)
print(dft.dtypes)
#change ytrain to one column->implemenet kneighborsregressor, or add the cities back to the test column and then implement classification

dft=dft.astype(float) #REGRESSION STARTS HERE
dfy = dft['Happiness']
dfx = dft.drop(axis=1, columns=['Happiness'])
print(dft.shape, dfx.shape, dfy.shape)

print(dft.shape, dfx.shape, dfy.shape)
#print(dfy)

xtrain, xtest,  ytrain,ytest = train_test_split(dfx.to_numpy(), dfy.to_numpy(), test_size=0.10)

print(xtest.shape,xtrain.shape)
print(ytest.shape,ytrain.shape)

from sklearn.neighbors import KNeighborsRegressor#1
knr = KNeighborsRegressor(3)
knr.fit(xtrain, ytrain)
prediction=knr.predict(xtest)
expected=(ytest)
score=knr.score(xtest,ytest)
print(prediction[:10])
print(expected[:10])
print(score)
print(r2_score(prediction, expected))

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
scores = cross_val_score(estimator=knr, X=dfx.to_numpy(), y=dfy.to_numpy(), cv=5)
scores

print(f'Mean accuracy: {scores.mean()}')
print(f'Accuracy standard deviation: {scores.std()}')

from sklearn.linear_model import LinearRegression#2
linear_regression = LinearRegression()
linear_regression.fit(X=xtrain, y=ytrain)

print(linear_regression.coef_)
print(linear_regression.intercept_)

predictedr = linear_regression.predict(xtest)
expecteddf = ytest
for p, e in zip(predictedr[::5], expecteddf[::5]):print(f'predicted: {p:.2f}, expected: {e:.2f}')

kfold = KFold(n_splits=10, random_state=11, shuffle=True)
scores = cross_val_score(estimator=knr, X=dfx, y=dfy, cv=kfold)
print(scores)

from sklearn.metrics import mean_absolute_error
print(mean_absolute_error(predictedr,ytest), r2_score(predictedr, ytest))

from sklearn.svm import SVR#3
#dfxn = dfx['Annual avg. hours worked'].to_numpy().reshape(-1,1)
dfyn = dfy.to_numpy().reshape(-1,1)
#print(xtrain.shape)
xtrain,xtest, ytrain, ytest = train_test_split(dfx.to_numpy(), dfyn, test_size=0.2)
print(xtest.shape,xtrain.shape)
print(ytest.shape,ytrain.shape)

svr = SVR().fit(xtrain, ytrain.ravel())
area=svr.predict(xtest)
print(area, ytrain[:5])
print(r2_score(area, ytrain[:5]),
      mean_squared_error(area, ytrain[:5]), mean_absolute_error(area, ytrain[:5]))

from sklearn.ensemble import RandomForestRegressor#4
from sklearn.metrics import r2_score
xtrain, xtest,  ytrain,ytest = train_test_split(dfx.to_numpy(), dfy.to_numpy(), test_size=0.20)
forest = RandomForestRegressor(max_depth=2, random_state=0)
forest.fit(xtrain,ytrain)
fpred= forest.predict(xtest)
print(forest.score(xtest, ytest), r2_score(fpred, ytrain[:5]),
      mean_squared_error(fpred, ytrain[:5]), mean_absolute_error(fpred, ytrain[:5]))

estimators = {
     'KNeighborsRegressor': knr,
     'LinearRegression':LinearRegression(), 
    'SupportVectorRegression': SVR(),
 'RandomForestRegressor': RandomForestRegressor()}
for estimator_name, estimator_object in estimators.items():
    kfold = KFold(n_splits=10, random_state=11, shuffle=True)
    scores = cross_val_score(estimator=estimator_object, 
    X=dfx.to_numpy(), y=dfy.to_numpy(), cv=kfold)
    print(f'{estimator_name}:\n'
    +f'Scores:\n{scores};\n'
    +f'Average Mean Accuracy: {scores.mean()}\n'
    +f'Average Mean STD: {scores.std()}\n')

"""Based on 10 test:
KneighborsRegressor : most of the differences we are getting are not too far off, on average atleast half of the test came out relatively accuracte. So might not be the best test, lets continue.
LinearRegression:
We can see that typically our prediction falls very close to being near the linear regression line, symbolizing that atleast half the time when compared our data is relatively linear.
SupportVectorRegression:
For support vector regression we were only returned 4 positive values, for this exact algorithm we are calculating a line surrounding a linear line between two sets of data. The vector will be where the most correlated variables are, typically named tube. So when a negative value is returned this is not good, meaning there is little to no linear correlation between the compared data.
RandomForestRegressor:
Random Forest Regressor is an algorithm that uses many different regression algorithms to create lots of predictions from k amount of trees. If this algorithm preforms porly, which for us it relatively did. Then that is a good sign for me that our data set is too small to be able to accurately predict data throughout.

The data is simply not vast and linear enough to be able to create accurate predictions on average. Even after conducting multiple folds and test for every single column within the dataset, still there is just not enough, correlated, data to be able to make predictions using most regression models.

As we can see here, random forest regression and linear regression are the most accurate models. Where linear regression does typically return less negative predictions, random forest regression on average returns less outliers. Given that on average random forest regression is faster and returns less outliers, I would have to chose this algorithm for the highest level of accuracy based on my dataset.
"""

#here i thought to myself. Why only calculate the scores with Y=dataframe['happiness'] only
#so i made a loop that calculates all of the scores for any possible column of the dataframe 
for columnz in dft:#for any column in data frame
  dfty = dft[columnz]#y=df['columnname']
  dftx = dft.drop(axis=1, columns=columnz)#x=df[!=columnname]
  print(columnz)#print column that scores are for
  for estimator_name, estimator_object in estimators.items(): #estimator loop
    xtrain, xtest,  ytrain,ytest = train_test_split(dftx.to_numpy(), dfty.to_numpy(), test_size=0.20) #split data
    estimator_object.fit(xtrain, ytrain) #fit data
    pred = estimator_object.predict(xtest) #predict data
    print(f'{estimator_name}.score(={estimator_object.score(xtest, ytest)})\n'#estimator score
         +f'r2_score:{r2_score(pred, ytrain[:5])}\n'#r2 score
         +f'MSE:{mean_squared_error(pred, ytrain[:5])}\n'
         +f'MAE:{mean_absolute_error(pred, ytrain[:5])}\n')

#why not implement 10 fold cross validation for every single possible column in data frame?
for columnz in dft:
  dfty = dft[columnz]
  dftx = dft.drop(axis=1, columns=columnz)
  print(columnz)
  for estimator_name, estimator_object in estimators.items():
    kfold = KFold(n_splits=10, random_state=11, shuffle=True)
    scores = cross_val_score(estimator=estimator_object, 
    X=dfx.to_numpy(), y=dfy.to_numpy(), cv=kfold)
    print(f'{estimator_name:>20}:\n'
    +f'Scores:\n{scores:};\n'
    +f'Avg Mean Accuracy: {scores.mean()}\n'
    +f'Avg Mean STD: {scores.std()}\n')

#here is everything together
estimators = {
     'KNeighborsRegressor': knr,
     'LinearRegression':LinearRegression(), 
    'SupportVectorRegression': SVR(),
 'RandomForestRegressor': RandomForestRegressor()}
lis = list()
for columnz in dft:
  dfty = dft[columnz]
  dftx = dft.drop(axis=1, columns=columnz)
  print(columnz)
  lis.append(columnz)
  for estimator_name, estimator_object in estimators.items(): #estimator loop
    xtrain, xtest,  ytrain,ytest = train_test_split(dftx.to_numpy(), dfty.to_numpy(), test_size=0.20)
    estimator_object.fit(xtrain, ytrain) #fit data
    pred = estimator_object.predict(xtest)
    kfold = KFold(n_splits=10, random_state=11, shuffle=True)
    scores = cross_val_score(estimator=estimator_object, 
    X=dfx.to_numpy(), y=dfy.to_numpy(), cv=kfold)
    print(f'{estimator_name}.score(={estimator_object.score(xtest, ytest)})\n'#estimator score
         +f'Scores:\n{scores:};\n'
         +f'r2_score: {r2_score(pred, ytrain[:5])}\n'#r2 score
         +f'MSE: {mean_squared_error(pred, ytrain[:5])}\n'
         +f'MAE: {mean_absolute_error(pred, ytrain[:5])}\nKFOLD:\n'
         +f'Avg Mean Accuracy: {scores.mean()}\n'
         +f'Avg Mean STD: {scores.std()}\n')
    scr = estimator_object.score(xtest, ytest)
    lis.append(scr)
    lis.append(scores.mean())
print(lis)
          #split data

#scatter plots
#warning makes 100 scatter plots
#this shows us the support vector regression for every column in the data frame
#here we can see though that because of the wide spread data points, svr has problems correlating th
for column in dft[:]:
  for column2 in dft[:-1]:
    if(column!=column2):
      x=dft[column].to_numpy().reshape(-1,1)
      y=dft[column2].to_numpy().reshape(-1,1)
      xtrain,xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2)
      svr = SVR().fit(xtrain, ytrain.ravel())
      area=svr.predict(xtest)
      plt.scatter(xtrain, ytrain, s=5, color="blue", label="original")
      plt.plot(xtest, area, lw=2, color="red", label="fitted")
      #plt.legend()
      plt.xlabel(column)
      plt.ylabel(column2)
      plt.show()

from keras.models import Sequential
from keras.layers import Dense
import tensorflow as tf
normalizer = tf.keras.layers.Normalization(axis=-1)
from keras.wrappers.scikit_learn import KerasRegressor
from tensorflow.keras import layers
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
dfx = dfx.iloc[:, :8]
xtrain, xtest, ytrain, ytest = train_test_split(dfx.to_numpy(),dfy.to_numpy(),
                                                    test_size=0.20, random_state=42)
print(xtrain.shape,xtest.shape)
print(ytrain.shape,ytest.shape)

model = Sequential()
model.add(Dense(12, input_dim=8, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
print(model.summary())

# compile the keras model
model.compile(loss='mean_squared_error', optimizer=tf.optimizers.Adam(learning_rate=0.1))

history = model.fit(dfx, dfy, epochs=20, verbose=0, validation_split=0.2)

error = model.evaluate(dfx, dfy)

hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist.tail()

linear_model = tf.keras.Sequential([
    layers.Dense(units=1)
])
#linear_model.predict(ytrain[:10])

linear_model.compile(
    optimizer=tf.optimizers.Adam(learning_rate=0.1),
    loss='mean_absolute_error')

history = linear_model.fit(
    dfx,
    dfy,
    epochs=100,
    # Suppress logging.
    verbose=0,
    # Calculate validation results on 20% of the training data.
    validation_split = 0.2)

linear_model.evaluate(dfx, dfy, verbose=0)

linear_model.predict(dfx)

dmodel = tf.keras.Sequential([
      normalizer,
      layers.Dense(64, activation='relu'),
      layers.Dense(64, activation='relu'),
      layers.Dense(1)
  ])
dmodel.compile(loss='mean_absolute_error',
                optimizer=tf.keras.optimizers.Adam(0.001))

history = dmodel.fit(
    dfx,
    dfy,
    validation_split=0.2,
    verbose=0, epochs=100)

dmodel.evaluate(dfx, dfy, verbose=0)

dmodel.predict(dfx)

dfx = dft.drop(axis=1, columns=['Happiness'])
dfx = dfx.iloc[:, :9]
xtrain, xtest, ytrain, ytest = train_test_split(dfx.to_numpy(),dfy.to_numpy(),
                                                    test_size=0.20, random_state=42)
def buildmodel():
    model = Sequential()
    model.add(Dense(12, input_dim=9, kernel_initializer='normal', 
              activation='relu'))
    model.add(Dense(8, activation='relu'))
    model.add(Dense(1, kernel_initializer='normal'))
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model

estimator = KerasRegressor(build_fn=buildmodel, nb_epoch=100, batch_size=100, verbose=False)
kfold = KFold(n_splits=10)
results = cross_val_score(estimator, xtrain, ytrain, cv=kfold)
print("Results: %.2f (%.2f) MSE" % (results.mean(), results.std()))

estimator.fit(xtrain, ytrain)
prediction = estimator.predict(xtest)
print(ytest, prediction)

"""Although neural networks are best used when trying to predict via classification, still my regressive neural network was not too inaccurate. When compared to a couple of the regression algorithms from sklearn, our MSE on average were not too much different. This is an example of how a regressive neural network may not always be necessary for finding accurate predictions when using regression. Linear regression infact is typically more accurate than a linear regressive neural network. Based off the accuracy of both findings, and practicing with many different ways of cleaning my data, and broadening what I am predicting. I can only asses that my chosen dataset was not the most appropriate for this project as it does not give machine learning algorithms enough examples for each column to accurately predict values or labels. Per label there needs to be more data to train from due to all of the data not being related. As well as per value, there needs to be more data to get a more accurate prediction. Overall for regression, it seems as if neural networks are not more accurate than linear regression algorithms(linear regression, randomforest regressor). """

